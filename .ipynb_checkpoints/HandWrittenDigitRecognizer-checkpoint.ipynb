{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import classification_report \n",
    "import os\n",
    "print(os.listdir(\"input\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('C:/Users/HOME/Desktop/Kaggle Comp/HandWrittenDigitRecog/Train.csv')\n",
    "test = pd.read_csv('C:/Users/HOME/Desktop/Kaggle Comp/HandWrittenDigitRecog/Test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=train.drop(columns='label',axis=1)/255\n",
    "y=train.label\n",
    "test = test/255\n",
    "\n",
    "from scipy.sparse import coo_matrix\n",
    "X_sparse = coo_matrix(X)\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "X, X_sparse, y = shuffle(X, X_sparse, y, random_state=0)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.41460180\n",
      "Iteration 2, loss = 0.19160710\n",
      "Iteration 3, loss = 0.14662671\n",
      "Iteration 4, loss = 0.11850842\n",
      "Iteration 5, loss = 0.10268457\n",
      "Iteration 6, loss = 0.09138090\n",
      "Iteration 7, loss = 0.08101609\n",
      "Iteration 8, loss = 0.07489680\n",
      "Iteration 9, loss = 0.06972963\n",
      "Iteration 10, loss = 0.06476231\n",
      "Iteration 11, loss = 0.06075606\n",
      "Iteration 12, loss = 0.05853192\n",
      "Iteration 13, loss = 0.05508510\n",
      "Iteration 14, loss = 0.05382356\n",
      "Iteration 15, loss = 0.05188581\n",
      "Iteration 16, loss = 0.05081011\n",
      "Iteration 17, loss = 0.04890735\n",
      "Iteration 18, loss = 0.04765206\n",
      "Iteration 19, loss = 0.04719439\n",
      "Iteration 20, loss = 0.04660474\n",
      "Iteration 21, loss = 0.04594034\n",
      "Iteration 22, loss = 0.04521863\n",
      "Iteration 23, loss = 0.04428300\n",
      "Iteration 24, loss = 0.04448083\n",
      "Iteration 25, loss = 0.04360768\n",
      "Iteration 26, loss = 0.04319927\n",
      "Iteration 27, loss = 0.04290090\n",
      "Iteration 28, loss = 0.04239660\n",
      "Iteration 29, loss = 0.04206182\n",
      "Iteration 30, loss = 0.04161088\n",
      "Iteration 31, loss = 0.04161616\n",
      "Iteration 32, loss = 0.04137495\n",
      "Iteration 33, loss = 0.04145054\n",
      "Iteration 34, loss = 0.04085693\n",
      "Iteration 35, loss = 0.04070452\n",
      "Iteration 36, loss = 0.04023727\n",
      "Iteration 37, loss = 0.04034660\n",
      "Iteration 38, loss = 0.04004327\n",
      "Iteration 39, loss = 0.03980292\n",
      "Iteration 40, loss = 0.03969504\n",
      "Iteration 41, loss = 0.03938737\n",
      "Iteration 42, loss = 0.03933300\n",
      "Iteration 43, loss = 0.03914456\n",
      "Iteration 44, loss = 0.03904990\n",
      "Iteration 45, loss = 0.03891734\n",
      "Iteration 46, loss = 0.03875544\n",
      "Iteration 47, loss = 0.03867882\n",
      "Iteration 48, loss = 0.03847502\n",
      "Iteration 49, loss = 0.03845169\n",
      "Iteration 50, loss = 0.03844250\n",
      "Iteration 51, loss = 0.03836560\n",
      "Iteration 52, loss = 0.03814277\n",
      "Iteration 53, loss = 0.03806273\n",
      "Iteration 54, loss = 0.03800008\n",
      "Iteration 55, loss = 0.03779817\n",
      "Iteration 56, loss = 0.03783303\n",
      "Iteration 57, loss = 0.03767912\n",
      "Iteration 58, loss = 0.03774841\n",
      "Iteration 59, loss = 0.03762418\n",
      "Iteration 60, loss = 0.03741257\n",
      "Iteration 61, loss = 0.03761914\n",
      "Iteration 62, loss = 0.03746656\n",
      "Iteration 63, loss = 0.03726822\n",
      "Iteration 64, loss = 0.03723888\n",
      "Iteration 65, loss = 0.03706884\n",
      "Iteration 66, loss = 0.03717794\n",
      "Iteration 67, loss = 0.03700456\n",
      "Iteration 68, loss = 0.03701732\n",
      "Iteration 69, loss = 0.03682152\n",
      "Iteration 70, loss = 0.03688587\n",
      "Iteration 71, loss = 0.03688827\n",
      "Iteration 72, loss = 0.03686762\n",
      "Iteration 73, loss = 0.03679606\n",
      "Iteration 74, loss = 0.03665107\n",
      "Iteration 75, loss = 0.03672369\n",
      "Iteration 76, loss = 0.03667186\n",
      "Iteration 77, loss = 0.03658083\n",
      "Iteration 78, loss = 0.03650488\n",
      "Iteration 79, loss = 0.03647099\n",
      "Iteration 80, loss = 0.03634967\n",
      "Iteration 81, loss = 0.03651765\n",
      "Iteration 82, loss = 0.03634833\n",
      "Iteration 83, loss = 0.03632003\n",
      "Iteration 84, loss = 0.03639444\n",
      "Iteration 85, loss = 0.03625061\n",
      "Iteration 86, loss = 0.03623286\n",
      "Iteration 87, loss = 0.03632316\n",
      "Iteration 88, loss = 0.03615867\n",
      "Iteration 89, loss = 0.03631534\n",
      "Iteration 90, loss = 0.03612159\n",
      "Iteration 91, loss = 0.03616615\n",
      "Iteration 92, loss = 0.03602154\n",
      "Iteration 93, loss = 0.03605911\n",
      "Iteration 94, loss = 0.03598646\n",
      "Iteration 95, loss = 0.03599500\n",
      "Iteration 96, loss = 0.03608640\n",
      "Iteration 97, loss = 0.03591327\n",
      "Iteration 98, loss = 0.03592190\n",
      "Iteration 99, loss = 0.03586338\n",
      "Iteration 100, loss = 0.03583389\n",
      "Iteration 101, loss = 0.03600302\n",
      "Iteration 102, loss = 0.03584003\n",
      "Iteration 103, loss = 0.03590495\n",
      "Iteration 104, loss = 0.03573549\n",
      "Iteration 105, loss = 0.03576010\n",
      "Iteration 106, loss = 0.03580861\n",
      "Iteration 107, loss = 0.03570652\n",
      "Iteration 108, loss = 0.03568807\n",
      "Iteration 109, loss = 0.03580263\n",
      "Iteration 110, loss = 0.03570794\n",
      "Iteration 111, loss = 0.03555246\n",
      "Iteration 112, loss = 0.03556186\n",
      "Iteration 113, loss = 0.03565179\n",
      "Iteration 114, loss = 0.03574112\n",
      "Iteration 115, loss = 0.03578164\n",
      "Iteration 116, loss = 0.03568598\n",
      "Iteration 117, loss = 0.03552733\n",
      "Iteration 118, loss = 0.03570185\n",
      "Iteration 119, loss = 0.03556898\n",
      "Iteration 120, loss = 0.03536541\n",
      "Iteration 121, loss = 0.03551455\n",
      "Iteration 122, loss = 0.03561134\n",
      "Iteration 123, loss = 0.03540521\n",
      "Iteration 124, loss = 0.03544975\n",
      "Iteration 125, loss = 0.03540125\n",
      "Iteration 126, loss = 0.03534259\n",
      "Iteration 127, loss = 0.03539689\n",
      "Iteration 128, loss = 0.03541894\n",
      "Iteration 129, loss = 0.03536036\n",
      "Iteration 130, loss = 0.03547979\n",
      "Iteration 131, loss = 0.03545915\n",
      "Iteration 132, loss = 0.03535640\n",
      "Iteration 133, loss = 0.03534675\n",
      "Iteration 134, loss = 0.03531662\n",
      "Iteration 135, loss = 0.03527739\n",
      "Iteration 136, loss = 0.03524000\n",
      "Iteration 137, loss = 0.03536704\n",
      "Iteration 138, loss = 0.03520909\n",
      "Iteration 139, loss = 0.03526742\n",
      "Iteration 140, loss = 0.03527431\n",
      "Iteration 141, loss = 0.03517781\n",
      "Iteration 142, loss = 0.03532404\n",
      "Iteration 143, loss = 0.03521592\n",
      "Iteration 144, loss = 0.03507437\n",
      "Iteration 145, loss = 0.03529778\n",
      "Iteration 146, loss = 0.03514759\n",
      "Iteration 147, loss = 0.03538374\n",
      "Iteration 148, loss = 0.03516331\n",
      "Iteration 149, loss = 0.03519400\n",
      "Iteration 150, loss = 0.03523873\n",
      "Iteration 151, loss = 0.03500018\n",
      "Iteration 152, loss = 0.03518647\n",
      "Iteration 153, loss = 0.03521555\n",
      "Iteration 154, loss = 0.03511352\n",
      "Iteration 155, loss = 0.03506569\n",
      "Iteration 156, loss = 0.03506240\n",
      "Iteration 157, loss = 0.03514598\n",
      "Iteration 158, loss = 0.03497125\n",
      "Iteration 159, loss = 0.03499539\n",
      "Iteration 160, loss = 0.03502417\n",
      "Iteration 161, loss = 0.03498387\n",
      "Iteration 162, loss = 0.03494848\n",
      "Iteration 163, loss = 0.03496180\n",
      "Iteration 164, loss = 0.03504552\n",
      "Iteration 165, loss = 0.03494189\n",
      "Iteration 166, loss = 0.03503952\n",
      "Iteration 167, loss = 0.03491369\n",
      "Iteration 168, loss = 0.03493544\n",
      "Iteration 169, loss = 0.03503086\n",
      "Iteration 170, loss = 0.03492317\n",
      "Iteration 171, loss = 0.03489973\n",
      "Iteration 172, loss = 0.03496829\n",
      "Iteration 173, loss = 0.03488580\n",
      "Iteration 174, loss = 0.03493848\n",
      "Iteration 175, loss = 0.03484158\n",
      "Iteration 176, loss = 0.03492011\n",
      "Iteration 177, loss = 0.03482537\n",
      "Iteration 178, loss = 0.03481060\n",
      "Iteration 179, loss = 0.03475652\n",
      "Iteration 180, loss = 0.03484284\n",
      "Iteration 181, loss = 0.03473843\n",
      "Iteration 182, loss = 0.03485136\n",
      "Iteration 183, loss = 0.03485206\n",
      "Iteration 184, loss = 0.03498576\n",
      "Iteration 185, loss = 0.03480352\n",
      "Iteration 186, loss = 0.03490617\n",
      "Iteration 187, loss = 0.03486453\n",
      "Iteration 188, loss = 0.03469343\n",
      "Iteration 189, loss = 0.03484344\n",
      "Iteration 190, loss = 0.03482807\n",
      "Iteration 191, loss = 0.03485283\n",
      "Iteration 192, loss = 0.03480233\n",
      "Iteration 193, loss = 0.03477580\n",
      "Iteration 194, loss = 0.03471671\n",
      "Iteration 195, loss = 0.03477614\n",
      "Iteration 196, loss = 0.03482593\n",
      "Iteration 197, loss = 0.03468418\n",
      "Iteration 198, loss = 0.03479632\n",
      "Iteration 199, loss = 0.03476596\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      1249\n",
      "           1       0.98      0.99      0.98      1426\n",
      "           2       0.97      0.97      0.97      1253\n",
      "           3       0.97      0.95      0.96      1230\n",
      "           4       0.98      0.99      0.98      1231\n",
      "           5       0.97      0.97      0.97      1147\n",
      "           6       0.98      0.99      0.98      1250\n",
      "           7       0.98      0.97      0.97      1300\n",
      "           8       0.96      0.96      0.96      1237\n",
      "           9       0.96      0.96      0.96      1277\n",
      "\n",
      "    accuracy                           0.97     12600\n",
      "   macro avg       0.97      0.97      0.97     12600\n",
      "weighted avg       0.97      0.97      0.97     12600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100, ), \n",
    "                    max_iter=1000, alpha=0.03,\n",
    "                    solver='sgd', verbose=10, \n",
    "                    tol=0.00001, random_state=1,\n",
    "                    learning_rate_init=.1)\n",
    "clf.fit(X_train,y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9734920634920635"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ImageId  Label\n",
       "0        1      2\n",
       "1        2      0\n",
       "2        3      9\n",
       "3        4      9\n",
       "4        5      3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subpred = clf.predict(test)\n",
    "submission = pd.read_csv('C:/Users/HOME/Desktop/Kaggle Comp/HandWrittenDigitRecog/sample_submission.csv')\n",
    "submission.Label = subpred\n",
    "submission.to_csv('C:/Users/HOME/Desktop/Kaggle Comp/HandWrittenDigitRecog/sub.csv',index=False)\n",
    "submission.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
